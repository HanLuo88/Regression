{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import medical_lib as ml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2: Balanciertes Training aber unbalanciertes Predicten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen der Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('naive_latest_todesinterval_model3_Train_langlebig.csv') \n",
    "traindata = traindata.iloc[:, 3:] \n",
    "traindata = traindata.sample(frac=1, random_state=42).reset_index(drop=True) #Shuffle Rows and Reset Index\n",
    "train_features = traindata.iloc[:, :-1]\n",
    "train_class = traindata.iloc[:, -1]\n",
    "\n",
    "testdata = pd.read_csv('naive_latest_todesinterval_model3_Test_langlebig.csv') \n",
    "testdata = testdata.iloc[:, 3:] \n",
    "testdata = testdata.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "test_features = testdata.iloc[:, :-1]\n",
    "test_class = testdata.iloc[:, -1]\n",
    "test_class_array = np.array(test_class)\n",
    "\n",
    "result = pd.read_csv('automated_algorithmen.csv')\n",
    "result = result.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy:  0.6953405017921147 KNN Precision:  0.8900000082795781 KNN Recall:  0.6953405017921147 KNN F1-Score:  0.7744503862150921\n",
      "Tatsächlich:  0.7419354838709677 0.21428571428571427 0.75 0.3333333333333333\n",
      "Durchschnittliche Abweichung:  0.8422939068100358\n",
      "Standartabweichung der Abweichung:  1.3744479920296782\n"
     ]
    }
   ],
   "source": [
    "medKNN = KNeighborsClassifier(n_neighbors=4)\n",
    "medKNN.fit(train_features,train_class)\n",
    "knnYpred = medKNN.predict(test_features)\n",
    "accuracyKNN = accuracy_score(test_class, knnYpred)\n",
    "precisionKNN = precision_score(test_class, knnYpred, average='weighted')\n",
    "recallKNN = recall_score(test_class, knnYpred, average='weighted')\n",
    "f1scoreKNN = f1_score(test_class, knnYpred, average='weighted')\n",
    "print('KNN Accuracy: ', accuracyKNN, 'KNN Precision: ', precisionKNN, 'KNN Recall: ', recallKNN, 'KNN F1-Score: ', f1scoreKNN )\n",
    "pred_tot_lebendigknn = []\n",
    "actual_tot_lebendigknn = []\n",
    "abweichungknn = []\n",
    "for el in range(0, len(knnYpred)):\n",
    "    dist = abs(knnYpred[el] - test_class_array[el])\n",
    "    abweichungknn.append(dist)\n",
    "    if knnYpred[el] < 7:\n",
    "        pred_tot_lebendigknn.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigknn.append(0)\n",
    "    if test_class_array[el] < 7:\n",
    "        actual_tot_lebendigknn.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigknn.append(0)\n",
    "accuracyknn, precisionknn, recallknn, f1scoreknn = ml.scoring(pred_tot_lebendigknn, actual_tot_lebendigknn)\n",
    "print('Tatsächlich: ', accuracyknn, precisionknn, recallknn, f1scoreknn)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungknn))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungknn))\n",
    "# index = result.index[result['Modellierung'] == 'Model_5_version_2'].tolist()\n",
    "# result.at[index, 'KNN_precision'] = precisionknn\n",
    "# result.at[index, 'KNN_recall'] = recallknn\n",
    "# result.at[index, 'KNN_f1'] = f1scoreknn\n",
    "# result.to_csv('automated_algorithmen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Regression Accuracy:  0.7992831541218638 Log-Regression Precision:  0.8963283040034986 Log-Regression Recall:  0.7992831541218638 Log-Regression F1-Score:  0.8348970026389381\n",
      "Tatsächlich:  0.8422939068100358 0.3275862068965517 0.7916666666666666 0.4634146341463415\n",
      "Durchschnittliche Abweichung:  0.3978494623655914\n",
      "Standartabweichung der Abweichung:  0.831371259358181\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(train_features,train_class)\n",
    "lr_y_pred = lr_model.predict(test_features)\n",
    "lr_accuracyLogReg = accuracy_score(test_class, lr_y_pred)\n",
    "lr_precisionLogReg = precision_score(test_class, lr_y_pred, average='weighted')\n",
    "lr_recallLogReg = recall_score(test_class, lr_y_pred, average='weighted')\n",
    "lr_f1scoreLogReg = f1_score(test_class, lr_y_pred, average='weighted')\n",
    "print('Log-Regression Accuracy: ', lr_accuracyLogReg, 'Log-Regression Precision: ', lr_precisionLogReg, 'Log-Regression Recall: ', lr_recallLogReg, 'Log-Regression F1-Score: ', lr_f1scoreLogReg )\n",
    "pred_tot_lebendiglr = []\n",
    "actual_tot_lebendiglr = []\n",
    "abweichunglr = []\n",
    "for el in range(0, len(lr_y_pred)):\n",
    "    dist = abs(lr_y_pred[el] - test_class_array[el])\n",
    "    abweichunglr.append(dist)\n",
    "    if lr_y_pred[el] < 7:\n",
    "        pred_tot_lebendiglr.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendiglr.append(0)\n",
    "    if test_class_array[el] < 7:\n",
    "        actual_tot_lebendiglr.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendiglr.append(0)\n",
    "accuracylr, precisionlr, recalllr, f1scorelr = ml.scoring(pred_tot_lebendiglr, actual_tot_lebendiglr)\n",
    "print('Tatsächlich: ', accuracylr, precisionlr, recalllr, f1scorelr)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichunglr))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichunglr))\n",
    "# index = result.index[result['Modellierung'] == 'Model_5_version_2'].tolist()\n",
    "# result.at[index, 'Logistic_Regression_precision'] = precisionlr\n",
    "# result.at[index, 'Logistic_Regression_recall'] = recalllr\n",
    "# result.at[index, 'Logistic_Regression_f1'] = f1scorelr\n",
    "# result.to_csv('automated_algorithmen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical_DecTree Accuracy:  0.6523297491039427 DecTree Precision:  0.9033637508619687 DecTree Recall:  0.6523297491039427 DecTree F1-Score:  0.7528346331690516\n",
      "Tatsächlich:  0.7132616487455197 0.21428571428571427 0.875 0.34426229508196726\n",
      "Durchschnittliche Abweichung:  0.7670250896057348\n",
      "Standartabweichung der Abweichung:  1.209070603143112\n"
     ]
    }
   ],
   "source": [
    "medical_DecTree = DecisionTreeClassifier(random_state=15)\n",
    "medical_DecTree = medical_DecTree.fit(train_features,train_class)\n",
    "decTree_pred = medical_DecTree.predict(test_features)\n",
    "accuracyDecTree = accuracy_score(test_class, decTree_pred)\n",
    "precisionDecTree = precision_score(test_class, decTree_pred, average='weighted')\n",
    "recallDecTree = recall_score(test_class, decTree_pred, average='weighted')\n",
    "f1scoreDecTree = f1_score(test_class, decTree_pred, average='weighted')\n",
    "print('medical_DecTree Accuracy: ', accuracyDecTree, 'DecTree Precision: ', precisionDecTree, 'DecTree Recall: ', recallDecTree, 'DecTree F1-Score: ', f1scoreDecTree )\n",
    "pred_tot_lebendigdc = []\n",
    "actual_tot_lebendigdc = []\n",
    "abweichungdc = []\n",
    "for el in range(0, len(decTree_pred)):\n",
    "    dist = abs(decTree_pred[el] - test_class_array[el])\n",
    "    abweichungdc.append(dist)\n",
    "    if decTree_pred[el] < 7:\n",
    "        pred_tot_lebendigdc.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigdc.append(0)\n",
    "    if test_class_array[el] < 7:\n",
    "        actual_tot_lebendigdc.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigdc.append(0)\n",
    "accuracydc, precisiondc, recalldc, f1scoredc = ml.scoring(pred_tot_lebendigdc, actual_tot_lebendigdc)\n",
    "print('Tatsächlich: ', accuracydc, precisiondc, recalldc, f1scoredc)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungdc))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungdc))\n",
    "# index = result.index[result['Modellierung'] == 'Model_5_version_2'].tolist()\n",
    "# result.at[index, 'Decision_Tree_precision'] = precisiondc\n",
    "# result.at[index, 'Decision_Tree_recall'] = recalldc\n",
    "# result.at[index, 'Decision_Tree_f1'] = f1scoredc\n",
    "# result.to_csv('automated_algorithmen.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Estimator: 100  RF Accuracy:  0.7634408602150538 RF Precision:  0.9142674406524336 RF Recall:  0.7634408602150538 RF F1-Score:  0.8228621881067251\n",
      "Tatsächlich:  0.8207885304659498 0.3194444444444444 0.9583333333333334 0.4791666666666667\n",
      "Durchschnittliche Abweichung:  0.5053763440860215\n",
      "Standartabweichung der Abweichung:  0.9536609999415796\n"
     ]
    }
   ],
   "source": [
    "medical_RF = RandomForestClassifier(random_state=43)\n",
    "medical_RF.fit(train_features,train_class)\n",
    "rfPred = medical_RF.predict(test_features)\n",
    "accuracyRF = accuracy_score(test_class, rfPred)\n",
    "precisionRF = precision_score(test_class, rfPred, average='weighted')\n",
    "recallRF = recall_score(test_class, rfPred, average='weighted')\n",
    "f1scoreRF = f1_score(test_class, rfPred, average='weighted')\n",
    "print('Anzahl Estimator: 100 ', 'RF Accuracy: ', accuracyRF, 'RF Precision: ', precisionRF, 'RF Recall: ', recallRF, 'RF F1-Score: ', f1scoreRF )\n",
    "pred_tot_lebendigrf = []\n",
    "actual_tot_lebendigrf = []\n",
    "abweichungrf = []\n",
    "for el in range(0, len(rfPred)):\n",
    "    dist = abs(rfPred[el] - test_class_array[el])\n",
    "    abweichungrf.append(dist)\n",
    "    if rfPred[el] < 7:\n",
    "        pred_tot_lebendigrf.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigrf.append(0)\n",
    "    if test_class_array[el] < 7:\n",
    "        actual_tot_lebendigrf.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigrf.append(0)\n",
    "accuracyrf, precisionrf, recallrf, f1scorerf = ml.scoring(pred_tot_lebendigrf, actual_tot_lebendigrf)\n",
    "print('Tatsächlich: ', accuracyrf, precisionrf, recallrf, f1scorerf)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungrf))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungrf))\n",
    "# index = result.index[result['Modellierung'] == 'Model_5_version_2'].tolist()\n",
    "# result.at[index, 'Random_Forest_precision'] = precisionrf\n",
    "# result.at[index, 'Random_Forest_recall'] = recallrf\n",
    "# result.at[index, 'Random_Forest_f1'] = f1scorerf\n",
    "# result.to_csv('automated_algorithmen.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADABOOST:  Accuracy:  0.5734767025089605 Precision:  0.9111019043092115 Recall:  0.5734767025089605 f1-Score:  0.6918190553227291\n",
      "Tatsächlich:  0.6344086021505376 0.18548387096774194 0.9583333333333334 0.31081081081081086\n",
      "Durchschnittliche Abweichung:  0.7741935483870968\n",
      "Standartabweichung der Abweichung:  1.024387558177596\n"
     ]
    }
   ],
   "source": [
    "adamodel = AdaBoostClassifier()\n",
    "adamodel.fit(train_features,train_class)\n",
    "adamodel_prediction = adamodel.predict(test_features)\n",
    "adamodel_accuracy = accuracy_score(test_class, adamodel_prediction)\n",
    "adamodel_precision = precision_score(test_class, adamodel_prediction, average='weighted')\n",
    "adamodel_recall = recall_score(test_class, adamodel_prediction, average='weighted')\n",
    "adamodel_f1 = f1_score(test_class, adamodel_prediction, average='weighted')\n",
    "print('ADABOOST: ', 'Accuracy: ', adamodel_accuracy,'Precision: ', adamodel_precision,'Recall: ', adamodel_recall,'f1-Score: ', adamodel_f1)\n",
    "pred_tot_lebendigada = []\n",
    "actual_tot_lebendigada = []\n",
    "abweichungada = []\n",
    "for el in range(0, len(adamodel_prediction)):\n",
    "    dist = abs(adamodel_prediction[el] - test_class_array[el])\n",
    "    abweichungada.append(dist)\n",
    "    if adamodel_prediction[el] < 7:\n",
    "        pred_tot_lebendigada.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigada.append(0)\n",
    "    if test_class_array[el] < 7:\n",
    "        actual_tot_lebendigada.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigada.append(0)\n",
    "accuracyada, precisionada, recallada, f1scoreada = ml.scoring(pred_tot_lebendigada, actual_tot_lebendigada)\n",
    "print('Tatsächlich: ', accuracyada, precisionada, recallada, f1scoreada)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungada))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungada))\n",
    "# index = result.index[result['Modellierung'] == 'Model_5_version_2'].tolist()\n",
    "# result.at[index, 'ADABoost_precision'] = precisionada\n",
    "# result.at[index, 'ADABoost_recall'] = recallada\n",
    "# result.at[index, 'ADABoost_f1'] = f1scoreada\n",
    "# result.to_csv('automated_algorithmen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOOST:  Accuracy:  0.7562724014336918 Precision:  0.9185910917821237 Recall:  0.7562724014336918 F1-Score:  0.8248298160578862\n",
      "Tatsächlich:  0.8207885304659498 0.3194444444444444 0.9583333333333334 0.4791666666666667\n",
      "Durchschnittliche Abweichung:  0.5412186379928315\n",
      "Standartabweichung der Abweichung:  1.0356996590414844\n",
      "#################################################################################################\n"
     ]
    }
   ],
   "source": [
    "xgmodel = XGBClassifier(eval_metric='error')\n",
    "xgmodel.fit(train_features,train_class)\n",
    "xgboosted_prediction = xgmodel.predict(test_features)\n",
    "xgboosted_accuracy = accuracy_score(test_class, xgboosted_prediction)\n",
    "xgboosted_precision = precision_score(test_class, xgboosted_prediction, average='weighted')\n",
    "xgboosted_recall = recall_score(test_class, xgboosted_prediction, average='weighted')\n",
    "xgboosted_f1 = f1_score(test_class, xgboosted_prediction, average='weighted')\n",
    "print('XGBOOST: ', 'Accuracy: ', xgboosted_accuracy, 'Precision: ', xgboosted_precision, 'Recall: ', xgboosted_recall, 'F1-Score: ', xgboosted_f1)\n",
    "pred_tot_lebendigxg = []\n",
    "actual_tot_lebendigxg = []\n",
    "abweichungxg = []\n",
    "for el in range(0, len(xgboosted_prediction)):\n",
    "    dist = abs(xgboosted_prediction[el] - test_class_array[el])\n",
    "    abweichungxg.append(dist)\n",
    "    if xgboosted_prediction[el] < 7:\n",
    "        pred_tot_lebendigxg.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigxg.append(0)\n",
    "    if test_class_array[el] < 7:\n",
    "        actual_tot_lebendigxg.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigxg.append(0)\n",
    "accuracyxg, precisionxg, recallxg, f1scorexg = ml.scoring(pred_tot_lebendigxg, actual_tot_lebendigxg)\n",
    "print('Tatsächlich: ', accuracyxg, precisionxg, recallxg, f1scorexg)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungxg))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungxg))\n",
    "print('#################################################################################################')\n",
    "# index = result.index[result['Modellierung'] == 'Model_5_version_2'].tolist()\n",
    "# result.at[index, 'XGBoost_precision'] = precisionxg\n",
    "# result.at[index, 'XGBoost_recall'] = recallxg\n",
    "# result.at[index, 'XGBoost_f1'] = f1scorexg\n",
    "# result.to_csv('automated_algorithmen.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
