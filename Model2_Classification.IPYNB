{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from numpy import sort\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost.core import Booster\n",
    "from matplotlib import pyplot\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import medical_lib as ml\n",
    "import pandas as pd\n",
    "from statistics import mean, mode\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "from numpy.lib.function_base import average\n",
    "from sklearn.utils import multiclass\n",
    "from xgboost.sklearn import XGBRFClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-520, -200), (-199, 0), (1, 14), (15, 30), (31, 60), (61, 90), (91, 120), (121, 180), (181, 365), (366, 850), (851, 1650)]\n"
     ]
    }
   ],
   "source": [
    "intervalle = [(-520, -200),(-199, 0),(1, 14),(15, 30),(31, 60),(61,90),(91,120),(121,180),(181,365),(366,850),(851,1650)]\n",
    "print(intervalle)\n",
    "medDatamodel2 = pd.read_csv(\n",
    "    'model2_Classificationtable_intervalstatus.csv') #model2_Classificationtable_intervalstatus_TMP\n",
    "medDataCopy_model2 = medDatamodel2.copy()\n",
    "medDataCopy_model2 = medDataCopy_model2.iloc[:, 3:]\n",
    "medDataCopy_model2_Features_Selected = medDataCopy_model2.copy()\n",
    "\n",
    "med_class_model2 = medDataCopy_model2.iloc[:, -1]\n",
    "med_features_model2 = medDataCopy_model2.iloc[:, :-1]\n",
    "\n",
    "result = pd.read_csv('automated_algorithmen.csv')\n",
    "result = result.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufteilen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_features_train_model2, med_features_test_model2, med_class_train_model2, med_class_test_model2 = train_test_split(med_features_model2, med_class_model2, test_size=0.2, random_state=43, stratify=med_class_model2)\n",
    "med_class_test_array = np.array(med_class_test_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medKNN = KNeighborsClassifier(n_neighbors=4)\n",
    "medKNN.fit(med_features_train_model2, med_class_train_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  4 KNN Accuracy:  0.7238095238095238 KNN Precision:  0.8263531335928216 KNN Recall:  0.7238095238095238 KNN F1-Score:  0.7698329592984453\n"
     ]
    }
   ],
   "source": [
    "knnYpred = medKNN.predict(med_features_test_model2)\n",
    "accuracyKNN = accuracy_score(knnYpred, med_class_test_array)\n",
    "precisionKNN = precision_score(knnYpred, med_class_test_array, average='weighted')\n",
    "recallKNN = recall_score(knnYpred, med_class_test_array, average='weighted')\n",
    "f1scoreKNN = f1_score(knnYpred, med_class_test_array, average='weighted')\n",
    "print('K: ', 4, 'KNN Accuracy: ', accuracyKNN, 'KNN Precision: ', precisionKNN, 'KNN Recall: ', recallKNN, 'KNN F1-Score: ', f1scoreKNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN: Tatsächlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatsächlich:  0.7464285714285714 0.3563218390804598 0.16489361702127658 0.22545454545454544\n",
      "Durchschnittliche Abweichung:  0.7261904761904762\n",
      "Standartabweichung der Abweichung:  1.4078960195320034\n"
     ]
    }
   ],
   "source": [
    "pred_tot_lebendigknn = []\n",
    "actual_tot_lebendigknn = []\n",
    "abweichungknn = []\n",
    "for el in range(0, len(knnYpred)):\n",
    "    dist = abs(knnYpred[el] - med_class_test_array[el])\n",
    "    abweichungknn.append(dist)\n",
    "    if knnYpred[el] < 12:\n",
    "        pred_tot_lebendigknn.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigknn.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendigknn.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigknn.append(0)\n",
    "accuracyknn, precisionknn, recallknn, f1scoreknn = ml.scoring(pred_tot_lebendigknn, actual_tot_lebendigknn)\n",
    "print('Tatsächlich: ', accuracyknn, precisionknn, recallknn, f1scoreknn)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungknn))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungknn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN: Übertragen der Ergebnisse für Precision, Recall und F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = result.index[result['Modellierung'] == 'Model_2'].tolist()\n",
    "result.at[index, 'KNN_precision'] = precisionknn\n",
    "result.at[index, 'KNN_recall'] = recallknn\n",
    "result.at[index, 'KNN_f1'] = f1scoreknn\n",
    "result.to_csv('automated_algorithmen.csv')\n",
    "# pyplot.hist(abweichungknn)\n",
    "# pyplot.title('Häugifkeitsverteilung der Abweichungen: Logistic Regression')\n",
    "# pyplot.xlabel(\"Wert\")\n",
    "# pyplot.ylabel(\"Häufigkeit\")\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial', solver='newton-cg')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n",
    "lr_model.fit(med_features_train_model2, med_class_train_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Regression Accuracy:  0.7690476190476191 Log-Regression Precision:  0.9470972954156565 Log-Regression Recall:  0.7690476190476191 Log-Regression F1-Score:  0.846468708461876\n"
     ]
    }
   ],
   "source": [
    "lr_y_pred = lr_model.predict(med_features_test_model2)\n",
    "lr_accuracyLogReg = accuracy_score(lr_y_pred, med_class_test_array)\n",
    "lr_precisionLogReg = precision_score(\n",
    "    lr_y_pred, med_class_test_array, average='weighted')\n",
    "lr_recallLogReg = recall_score(\n",
    "    lr_y_pred, med_class_test_array, average='weighted')\n",
    "lr_f1scoreLogReg = f1_score(lr_y_pred, med_class_test_array, average='weighted')\n",
    "print('Log-Regression Accuracy: ', lr_accuracyLogReg, 'Log-Regression Precision: ', lr_precisionLogReg,\n",
    "      'Log-Regression Recall: ', lr_recallLogReg, 'Log-Regression F1-Score: ', lr_f1scoreLogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression: Tatsächlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatsächlich:  0.7845238095238095 0.6129032258064516 0.10106382978723404 0.1735159817351598\n",
      "Durchschnittliche Abweichung:  0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "pred_tot_lebendiglr = []\n",
    "actual_tot_lebendiglr = []\n",
    "abweichunglr = []\n",
    "for el in range(0, len(lr_y_pred)):\n",
    "    dist = abs(lr_y_pred[el] - med_class_test_array[el])\n",
    "    abweichunglr.append(dist)\n",
    "    if lr_y_pred[el] < 12:\n",
    "        pred_tot_lebendiglr.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendiglr.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendiglr.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendiglr.append(0)\n",
    "accuracylr, precisionlr, recalllr, f1scorelr = ml.scoring(pred_tot_lebendiglr, actual_tot_lebendiglr)\n",
    "print('Tatsächlich: ', accuracylr, precisionlr, recalllr, f1scorelr)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichunglr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression: Übertragen der Ergebnisse Für Precision, Recall und F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = result.index[result['Modellierung'] == 'Model_2'].tolist()\n",
    "result.at[index, 'Logistic_Regression_precision'] = precisionlr\n",
    "result.at[index, 'Logistic_Regression_recall'] = recalllr\n",
    "result.at[index, 'Logistic_Regression_f1'] = f1scorelr\n",
    "result.to_csv('automated_algorithmen.csv')\n",
    "# pyplot.hist(abweichunglr)\n",
    "# pyplot.title('Häugifkeitsverteilung der Abweichungen: Logistic Regression')\n",
    "# pyplot.xlabel(\"Wert\")\n",
    "# pyplot.ylabel(\"Häufigkeit\")\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_DecTree = DecisionTreeClassifier(random_state=17)\n",
    "medical_DecTree = medical_DecTree.fit(med_features_train_model2, med_class_train_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:  medical_DecTree Accuracy:  0.6583333333333333 DecTree Precision:  0.6597324005359719 DecTree Recall:  0.6583333333333333 DecTree F1-Score:  0.6589031578813412\n"
     ]
    }
   ],
   "source": [
    "decTree_pred = medical_DecTree.predict(med_features_test_model2)\n",
    "accuracyDecTree = accuracy_score(decTree_pred, med_class_test_array)\n",
    "precisionDecTree = precision_score(\n",
    "    decTree_pred, med_class_test_array, average='weighted')\n",
    "recallDecTree = recall_score(\n",
    "    decTree_pred, med_class_test_array, average='weighted')\n",
    "f1scoreDecTree = f1_score(decTree_pred, med_class_test_array, average='weighted')\n",
    "print('Decision Tree: ', 'medical_DecTree Accuracy: ', accuracyDecTree, 'DecTree Precision: ',\n",
    "      precisionDecTree, 'DecTree Recall: ', recallDecTree, 'DecTree F1-Score: ', f1scoreDecTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree: Tatsächlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatsächlich:  0.7142857142857143 0.3617021276595745 0.3617021276595745 0.3617021276595745\n",
      "Durchschnittliche Abweichung:  0.8654761904761905\n",
      "Standartabweichung der Abweichung:  1.4325297343892367\n"
     ]
    }
   ],
   "source": [
    "pred_tot_lebendigdc = []\n",
    "actual_tot_lebendigdc = []\n",
    "abweichungdc = []\n",
    "for el in range(0, len(decTree_pred)):\n",
    "    dist = abs(decTree_pred[el] - med_class_test_array[el])\n",
    "    abweichungdc.append(dist)\n",
    "    if decTree_pred[el] < 12:\n",
    "        pred_tot_lebendigdc.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigdc.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendigdc.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigdc.append(0)\n",
    "accuracydc, precisiondc, recalldc, f1scoredc = ml.scoring(pred_tot_lebendigdc, actual_tot_lebendigdc)\n",
    "print('Tatsächlich: ', accuracydc, precisiondc, recalldc, f1scoredc)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungdc))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungdc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree: Übertragen der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = result.index[result['Modellierung'] == 'Model_2'].tolist()\n",
    "result.at[index, 'Decision_Tree_precision'] = precisiondc\n",
    "result.at[index, 'Decision_Tree_recall'] = recalldc\n",
    "result.at[index, 'Decision_Tree_f1'] = f1scoredc\n",
    "result.to_csv('automated_algorithmen.csv')\n",
    "# pyplot.hist(abweichungdc)\n",
    "# pyplot.title('Häugifkeitsverteilung der Abweichungen: Decision Tree')\n",
    "# pyplot.xlabel(\"Wert\")\n",
    "# pyplot.ylabel(\"Häufigkeit\")\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical_RF = RandomForestClassifier(\n",
    "    n_estimators=100, random_state= 20)\n",
    "medical_RF.fit(med_features_train_model2, med_class_train_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Estimator: 100  RF Accuracy:  0.7785714285714286 RF Precision:  0.9892308389706506 RF Recall:  0.7785714285714286 RF F1-Score:  0.8695010875167587\n"
     ]
    }
   ],
   "source": [
    "rfPred = medical_RF.predict(med_features_test_model2)\n",
    "accuracyRF = accuracy_score(rfPred, med_class_test_array)\n",
    "precisionRF = precision_score(rfPred, med_class_test_array, average='weighted')\n",
    "recallRF = recall_score(rfPred, med_class_test_array, average='weighted')\n",
    "f1scoreRF = f1_score(rfPred, med_class_test_array, average='weighted')\n",
    "print('Anzahl Estimator: 100 ', 'RF Accuracy: ', accuracyRF, 'RF Precision: ',\n",
    "      precisionRF, 'RF Recall: ', recallRF, 'RF F1-Score: ', f1scoreRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest: Tatsächlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatsächlich:  0.7833333333333333 0.875 0.03723404255319149 0.07142857142857141\n",
      "Durchschnittliche Abweichung:  0.6035714285714285\n",
      "Standartabweichung der Abweichung:  1.3233207173956112\n"
     ]
    }
   ],
   "source": [
    "pred_tot_lebendigrf = []\n",
    "actual_tot_lebendigrf = []\n",
    "abweichungrf = []\n",
    "for el in range(0, len(rfPred)):\n",
    "    dist = abs(rfPred[el] - med_class_test_array[el])\n",
    "    abweichungrf.append(dist)\n",
    "    if rfPred[el] < 12:\n",
    "        pred_tot_lebendigrf.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigrf.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendigrf.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigrf.append(0)\n",
    "accuracyrf, precisionrf, recallrf, f1scorerf = ml.scoring(pred_tot_lebendigrf, actual_tot_lebendigrf)\n",
    "print('Tatsächlich: ', accuracyrf, precisionrf, recallrf, f1scorerf)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungrf))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest: Übertragen der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = result.index[result['Modellierung'] == 'Model_2'].tolist()\n",
    "result.at[index, 'Random_Forest_precision'] = precisionrf\n",
    "result.at[index, 'Random_Forest_recall'] = recallrf\n",
    "result.at[index, 'Random_Forest_f1'] = f1scorerf\n",
    "result.to_csv('automated_algorithmen.csv')\n",
    "# pyplot.hist(abweichungrf)\n",
    "# pyplot.title('Häugifkeitsverteilung der Abweichungen: Random Forest')\n",
    "# pyplot.xlabel(\"Wert\")\n",
    "# pyplot.ylabel(\"Häufigkeit\")\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADABoost: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adamodel = AdaBoostClassifier()\n",
    "adamodel.fit(med_features_train_model2, med_class_train_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADABoost: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADABOOST:  Accuracy:  0.7761904761904762 Precision:  0.6024716553287982 Recall:  0.7761904761904762 f1-Score:  0.6783863143112473\n"
     ]
    }
   ],
   "source": [
    "adamodel_prediction = adamodel.predict(med_features_test_model2)\n",
    "adamodel_accuracy = accuracy_score(med_class_test_model2, adamodel_prediction)\n",
    "adamodel_precision = precision_score(\n",
    "    med_class_test_model2, adamodel_prediction, average='weighted')\n",
    "adamodel_recall = recall_score(\n",
    "    med_class_test_model2, adamodel_prediction, average='weighted')\n",
    "adamodel_f1 = f1_score(med_class_test_model2,\n",
    "                       adamodel_prediction, average='weighted')\n",
    "print('ADABOOST: ', 'Accuracy: ', adamodel_accuracy, 'Precision: ',\n",
    "      adamodel_precision, 'Recall: ', adamodel_recall, 'f1-Score: ', adamodel_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADABoost: Tatsächlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: Division by Zero\n",
      "f1score: Division by Zero\n",
      "Division by Zero\n"
     ]
    }
   ],
   "source": [
    "pred_tot_lebendigada = []\n",
    "actual_tot_lebendigada = []\n",
    "abweichungada = []\n",
    "for el in range(0, len(adamodel_prediction)):\n",
    "    dist = abs(adamodel_prediction[el] - med_class_test_array[el])\n",
    "    abweichungada.append(dist)\n",
    "    if adamodel_prediction[el] < 12:\n",
    "        pred_tot_lebendigada.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigada.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendigada.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigada.append(0)\n",
    "try:\n",
    "    accuracyada, precisionada, recallada, f1scoreada = ml.scoring(pred_tot_lebendigada, actual_tot_lebendigada)\n",
    "    print('Tatsächlich: ', accuracyada, precisionada, recallada, f1scoreada)\n",
    "    print('Durchschnittliche Abweichung: ', np.mean(abweichungada))\n",
    "    print('Standartabweichung der Abweichung: ', np.std(abweichungada))\n",
    "except:\n",
    "    precisionada = 0.0\n",
    "    recallada = 0.0\n",
    "    f1scoreada = 0.0\n",
    "    print('Division by Zero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADABoost: Übertragen der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = result.index[result['Modellierung'] == 'Model_2'].tolist()\n",
    "result.at[index, 'ADABoost_precision'] = precisionada\n",
    "result.at[index, 'ADABoost_recall'] = recallada\n",
    "result.at[index, 'ADABoost_f1'] = f1scoreada\n",
    "result.to_csv('automated_algorithmen.csv')\n",
    "# pyplot.hist(abweichungada)\n",
    "# pyplot.title('Häugifkeitsverteilung der Abweichungen: ADABoost')\n",
    "# pyplot.xlabel(\"Wert\")\n",
    "# pyplot.ylabel(\"Häufigkeit\")\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='mlogloss', gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgmodel = XGBClassifier(eval_metric='mlogloss')\n",
    "xgmodel.fit(med_features_train_model2, med_class_train_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOOST:  Accuracy:  0.780952380952381 Precision:  0.7049249567813924 Recall:  0.780952380952381 F1-Score:  0.7063880099285258\n"
     ]
    }
   ],
   "source": [
    "xgboosted_prediction = xgmodel.predict(med_features_test_model2)\n",
    "xgboosted_accuracy = accuracy_score(\n",
    "    med_class_test_model2, xgboosted_prediction)\n",
    "xgboosted_precision = precision_score(\n",
    "    med_class_test_model2, xgboosted_prediction, average='weighted')\n",
    "xgboosted_recall = recall_score(\n",
    "    med_class_test_model2, xgboosted_prediction, average='weighted')\n",
    "xgboosted_f1 = f1_score(med_class_test_model2,\n",
    "                        xgboosted_prediction, average='weighted')\n",
    "print('XGBOOST: ', 'Accuracy: ', xgboosted_accuracy, 'Precision: ',\n",
    "      xgboosted_precision, 'Recall: ', xgboosted_recall, 'F1-Score: ', xgboosted_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost: Tatsächlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatsächlich:  0.7976190476190477 0.78125 0.13297872340425532 0.22727272727272727\n",
      "Durchschnittliche Abweichung:  0.5857142857142857\n",
      "Standartabweichung der Abweichung:  1.2881458488428847\n"
     ]
    }
   ],
   "source": [
    "pred_tot_lebendigxg = []\n",
    "actual_tot_lebendigxg = []\n",
    "abweichungxg = []\n",
    "for el in range(0, len(xgboosted_prediction)):\n",
    "    dist = abs(xgboosted_prediction[el] - med_class_test_array[el])\n",
    "    abweichungxg.append(dist)\n",
    "    if xgboosted_prediction[el] < 12:\n",
    "        pred_tot_lebendigxg.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigxg.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendigxg.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigxg.append(0)\n",
    "\n",
    "accuracyxg, precisionxg, recallxg, f1scorexg = ml.scoring(pred_tot_lebendigxg, actual_tot_lebendigxg)\n",
    "print('Tatsächlich: ', accuracyxg, precisionxg, recallxg, f1scorexg)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungxg))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungxg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost: Übertragen der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = result.index[result['Modellierung'] == 'Model_2'].tolist()\n",
    "result.at[index, 'XGBoost_precision'] = precisionxg\n",
    "result.at[index, 'XGBoost_recall'] = recallxg\n",
    "result.at[index, 'XGBoost_f1'] = f1scorexg\n",
    "result.to_csv('automated_algorithmen.csv')\n",
    "# pyplot.hist(abweichungxg)\n",
    "# pyplot.title('Häugifkeitsverteilung der Abweichungen: XGBoost')\n",
    "# pyplot.xlabel(\"Wert\")\n",
    "# pyplot.ylabel(\"Häufigkeit\")\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost: Feature-Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureranking = sorted((value, key) for (key, value) in xgmodel.get_booster().get_score(importance_type= 'gain').items())\n",
    "# pyplot.rcParams['figure.figsize'] = [30,30]\n",
    "# plot_importance(xgmodel.get_booster().get_score(importance_type= 'gain'))\n",
    "# pyplot.show()\n",
    "\n",
    "newfeatures = []\n",
    "for i in range(len(featureranking)):\n",
    "    if featureranking[i][0] < 1.0:\n",
    "        newfeatures.append(featureranking[i][1])\n",
    "# print(newfeatures)\n",
    "for el in newfeatures:\n",
    "    medDataCopy_model2.drop(el, inplace=True, axis=1)\n",
    "medDataCopy_model2.to_csv('medDataCopy_model2_Features_Selected.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Feature-Selection: Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-520, -200), (-199, 0), (1, 14), (15, 30), (31, 60), (61, 90), (91, 120), (121, 180), (181, 365), (366, 850), (851, 1650)]\n"
     ]
    }
   ],
   "source": [
    "intervalle = [(-520, -200),(-199, 0),(1, 14),(15, 30),(31, 60),(61,90),(91,120),(121,180),(181,365),(366,850),(851,1650)]\n",
    "print(intervalle)\n",
    "medDatamodel2 = pd.read_csv(\n",
    "    'medDataCopy_model2_Features_Selected.csv') #model2_Classificationtable_intervalstatus_TMP\n",
    "medDataCopy_model2 = medDatamodel2.copy()\n",
    "medDataCopy_model2 = medDataCopy_model2.iloc[:, 1:]\n",
    "medDataCopy_model2_Features_Selected = medDataCopy_model2.copy()\n",
    "\n",
    "med_class_model2 = medDataCopy_model2.iloc[:, -1]\n",
    "med_features_model2 = medDataCopy_model2.iloc[:, :-1]\n",
    "med_features_train_model2, med_features_test_model2, med_class_train_model2, med_class_test_model2 = train_test_split(med_features_model2, med_class_model2, test_size=0.2, random_state=43, stratify=med_class_model2)\n",
    "med_class_test_array = np.array(med_class_test_model2)\n",
    "\n",
    "result = pd.read_csv('automated_algorithmen.csv')\n",
    "result = result.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatsächlich:  0.7488095238095238 0.3707865168539326 0.17553191489361702 0.2382671480144405\n",
      "Durchschnittliche Abweichung:  0.7333333333333333\n",
      "Standartabweichung der Abweichung:  1.4434856402648395\n"
     ]
    }
   ],
   "source": [
    "medKNN = KNeighborsClassifier(n_neighbors=4)\n",
    "medKNN.fit(med_features_train_model2, med_class_train_model2)\n",
    "knnYpred = medKNN.predict(med_features_test_model2)\n",
    "pred_tot_lebendigknn = []\n",
    "actual_tot_lebendigknn = []\n",
    "abweichungknn = []\n",
    "for el in range(0, len(knnYpred)):\n",
    "    dist = abs(knnYpred[el] - med_class_test_array[el])\n",
    "    abweichungknn.append(dist)\n",
    "    if knnYpred[el] < 12:\n",
    "        pred_tot_lebendigknn.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigknn.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendigknn.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigknn.append(0)\n",
    "accuracyknn, precisionknn, recallknn, f1scoreknn = ml.scoring(pred_tot_lebendigknn, actual_tot_lebendigknn)\n",
    "print('Tatsächlich: ', accuracyknn, precisionknn, recallknn, f1scoreknn)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungknn))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungknn))\n",
    "index = result.index[result['Modellierung'] == 'Model_2_selected'].tolist()\n",
    "result.at[index, 'KNN_precision'] = precisionknn\n",
    "result.at[index, 'KNN_recall'] = recallknn\n",
    "result.at[index, 'KNN_f1'] = f1scoreknn\n",
    "result.to_csv('automated_algorithmen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatsächlich:  0.7869047619047619 0.6666666666666666 0.09574468085106383 0.16744186046511628\n",
      "Durchschnittliche Abweichung:  0.6345238095238095\n",
      "Standartabweichung der Abweichung:  1.355236233810588\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n",
    "lr_model.fit(med_features_train_model2, med_class_train_model2)\n",
    "lr_y_pred = lr_model.predict(med_features_test_model2)\n",
    "pred_tot_lebendiglr = []\n",
    "actual_tot_lebendiglr = []\n",
    "abweichunglr = []\n",
    "for el in range(0, len(lr_y_pred)):\n",
    "    dist = abs(lr_y_pred[el] - med_class_test_array[el])\n",
    "    abweichunglr.append(dist)\n",
    "    if lr_y_pred[el] < 12:\n",
    "        pred_tot_lebendiglr.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendiglr.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendiglr.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendiglr.append(0)\n",
    "accuracylr, precisionlr, recalllr, f1scorelr = ml.scoring(pred_tot_lebendiglr, actual_tot_lebendiglr)\n",
    "print('Tatsächlich: ', accuracylr, precisionlr, recalllr, f1scorelr)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichunglr))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichunglr))\n",
    "index = result.index[result['Modellierung'] == 'Model_2_selected'].tolist()\n",
    "result.at[index, 'Logistic_Regression_precision'] = precisionlr\n",
    "result.at[index, 'Logistic_Regression_recall'] = recalllr\n",
    "result.at[index, 'Logistic_Regression_f1'] = f1scorelr\n",
    "result.to_csv('automated_algorithmen.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatsächlich:  0.7047619047619048 0.34375 0.35106382978723405 0.3473684210526316\n",
      "Durchschnittliche Abweichung:  0.9333333333333333\n",
      "Standartabweichung der Abweichung:  1.5415926623164553\n"
     ]
    }
   ],
   "source": [
    "medical_DecTree = DecisionTreeClassifier(random_state=17)\n",
    "medical_DecTree = medical_DecTree.fit(med_features_train_model2, med_class_train_model2)\n",
    "decTree_pred = medical_DecTree.predict(med_features_test_model2)\n",
    "pred_tot_lebendigdc = []\n",
    "actual_tot_lebendigdc = []\n",
    "abweichungdc = []\n",
    "for el in range(0, len(decTree_pred)):\n",
    "    dist = abs(decTree_pred[el] - med_class_test_array[el])\n",
    "    abweichungdc.append(dist)\n",
    "    if decTree_pred[el] < 12:\n",
    "        pred_tot_lebendigdc.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigdc.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendigdc.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigdc.append(0)\n",
    "accuracydc, precisiondc, recalldc, f1scoredc = ml.scoring(pred_tot_lebendigdc, actual_tot_lebendigdc)\n",
    "print('Tatsächlich: ', accuracydc, precisiondc, recalldc, f1scoredc)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungdc))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungdc))\n",
    "index = result.index[result['Modellierung'] == 'Model_2_selected'].tolist()\n",
    "result.at[index, 'Decision_Tree_precision'] = precisiondc\n",
    "result.at[index, 'Decision_Tree_recall'] = recalldc\n",
    "result.at[index, 'Decision_Tree_f1'] = f1scoredc\n",
    "result.to_csv('automated_algorithmen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatsächlich:  0.7952380952380952 0.8636363636363636 0.10106382978723404 0.18095238095238092\n",
      "Durchschnittliche Abweichung:  0.5833333333333334\n",
      "Standartabweichung der Abweichung:  1.2947543739601304\n"
     ]
    }
   ],
   "source": [
    "medical_RF = RandomForestClassifier(\n",
    "    n_estimators=100, random_state= 20)\n",
    "medical_RF.fit(med_features_train_model2, med_class_train_model2)\n",
    "rfPred = medical_RF.predict(med_features_test_model2)\n",
    "pred_tot_lebendigrf = []\n",
    "actual_tot_lebendigrf = []\n",
    "abweichungrf = []\n",
    "for el in range(0, len(rfPred)):\n",
    "    dist = abs(rfPred[el] - med_class_test_array[el])\n",
    "    abweichungrf.append(dist)\n",
    "    if rfPred[el] < 12:\n",
    "        pred_tot_lebendigrf.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigrf.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendigrf.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigrf.append(0)\n",
    "accuracyrf, precisionrf, recallrf, f1scorerf = ml.scoring(pred_tot_lebendigrf, actual_tot_lebendigrf)\n",
    "print('Tatsächlich: ', accuracyrf, precisionrf, recallrf, f1scorerf)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungrf))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungrf))\n",
    "index = result.index[result['Modellierung'] == 'Model_2_selected'].tolist()\n",
    "result.at[index, 'Random_Forest_precision'] = precisionrf\n",
    "result.at[index, 'Random_Forest_recall'] = recallrf\n",
    "result.at[index, 'Random_Forest_f1'] = f1scorerf\n",
    "result.to_csv('automated_algorithmen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: Division by Zero\n",
      "f1score: Division by Zero\n",
      "Division by Zero\n"
     ]
    }
   ],
   "source": [
    "adamodel = AdaBoostClassifier()\n",
    "adamodel.fit(med_features_train_model2, med_class_train_model2)\n",
    "adamodel_prediction = adamodel.predict(med_features_test_model2)\n",
    "pred_tot_lebendigada = []\n",
    "actual_tot_lebendigada = []\n",
    "abweichungada = []\n",
    "for el in range(0, len(adamodel_prediction)):\n",
    "    dist = abs(adamodel_prediction[el] - med_class_test_array[el])\n",
    "    abweichungada.append(dist)\n",
    "    if adamodel_prediction[el] < 12:\n",
    "        pred_tot_lebendigada.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigada.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendigada.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigada.append(0)\n",
    "try:\n",
    "    accuracyada, precisionada, recallada, f1scoreada = ml.scoring(pred_tot_lebendigada, actual_tot_lebendigada)\n",
    "    print('Tatsächlich: ', accuracyada, precisionada, recallada, f1scoreada)\n",
    "    print('Durchschnittliche Abweichung: ', np.mean(abweichungada))\n",
    "    print('Standartabweichung der Abweichung: ', np.std(abweichungada))\n",
    "except:\n",
    "    precisionada = 0.0\n",
    "    print('Division by Zero')\n",
    "index = result.index[result['Modellierung'] == 'Model_2_selected'].tolist()\n",
    "result.at[index, 'ADABoost_precision'] = precisionada\n",
    "result.at[index, 'ADABoost_recall'] = recallada\n",
    "result.at[index, 'ADABoost_f1'] = f1scoreada\n",
    "result.to_csv('automated_algorithmen.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tatsächlich:  0.7988095238095239 0.7021276595744681 0.17553191489361702 0.28085106382978725\n",
      "Durchschnittliche Abweichung:  0.6071428571428571\n",
      "Standartabweichung der Abweichung:  1.332897462769925\n"
     ]
    }
   ],
   "source": [
    "xgmodel = XGBClassifier(eval_metric='mlogloss')\n",
    "xgmodel.fit(med_features_train_model2, med_class_train_model2)\n",
    "xgboosted_prediction = xgmodel.predict(med_features_test_model2)\n",
    "pred_tot_lebendigxg = []\n",
    "actual_tot_lebendigxg = []\n",
    "abweichungxg = []\n",
    "for el in range(0, len(xgboosted_prediction)):\n",
    "    dist = abs(xgboosted_prediction[el] - med_class_test_array[el])\n",
    "    abweichungxg.append(dist)\n",
    "    if xgboosted_prediction[el] < 12:\n",
    "        pred_tot_lebendigxg.append(1)\n",
    "    else: \n",
    "        pred_tot_lebendigxg.append(0)\n",
    "    if med_class_test_array[el] < 12:\n",
    "        actual_tot_lebendigxg.append(1)\n",
    "    else:\n",
    "        actual_tot_lebendigxg.append(0)\n",
    "\n",
    "accuracyxg, precisionxg, recallxg, f1scorexg = ml.scoring(pred_tot_lebendigxg, actual_tot_lebendigxg)\n",
    "print('Tatsächlich: ', accuracyxg, precisionxg, recallxg, f1scorexg)\n",
    "print('Durchschnittliche Abweichung: ', np.mean(abweichungxg))\n",
    "print('Standartabweichung der Abweichung: ', np.std(abweichungxg))\n",
    "index = result.index[result['Modellierung'] == 'Model_2_selected'].tolist()\n",
    "result.at[index, 'XGBoost_precision'] = precisionxg\n",
    "result.at[index, 'XGBoost_recall'] = recallxg\n",
    "result.at[index, 'XGBoost_f1'] = f1scorexg\n",
    "result.to_csv('automated_algorithmen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
